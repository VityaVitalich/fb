{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting VV_TextAnalyzer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile VV_TextAnalyzer.py\n",
    "\n",
    "\n",
    "\n",
    "#basics\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import collections\n",
    "import re\n",
    "import functools\n",
    "import operator\n",
    "from datetime import datetime, timedelta\n",
    "from accessify import protected\n",
    "from copy import deepcopy\n",
    "\n",
    "#NLP\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "nltk.download(\"stopwords\")\n",
    "from tqdm import tqdm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import gensim\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk import ngrams\n",
    "\n",
    "#visualise\n",
    "import matplotlib.pyplot as plt\n",
    "import chart_studio.plotly as py\n",
    "#import plotly.graph_obs as go\n",
    "from plotly.offline import iplot\n",
    "import cufflinks as cf\n",
    "import matplotlib.cm as cm\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "#fastai\n",
    "from sklearn.model_selection import train_test_split\n",
    "import fastai\n",
    "from fastai.text.transform import Tokenizer\n",
    "from fastai.text.learner import text_classifier_learner\n",
    "from fastai.text.models import AWD_LSTM\n",
    "from fastai.text.models import awd_lstm_lm_config\n",
    "\n",
    "\n",
    "#extra\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "это модель тональности\n",
    "ее нужно вызывать вот так\n",
    "mod = SentimentalModel()\n",
    "mod.fit()\n",
    "далее она будет писать тебе что сформировала батчи и обучилась\n",
    "после этого можно предсказывать \n",
    "mod.predict('твое предложение')\n",
    "и он отдаст тебе тензор, если тот показыват 0, то негативное, если 1 позитивное сообщение\n",
    "с этим нужно быть аккуратным, там также сбоку циферки, которые в сумме дают 1, это вероятности позитивного\n",
    "или негативного\n",
    "посматривай на них обязательно\n",
    "а еще чем больше слов ты передашь, тем будет точнее\n",
    "\n",
    "ВАЖНО!\n",
    "в директории, в которой находится твой питоновский скрипт обязательно должны быть четыре файла\n",
    "негатив_сенти.цсв\n",
    "позитив_сенти.цсв\n",
    "тв_лстм\n",
    "фт_енк\n",
    "'''\n",
    "class SentimentalModel:\n",
    "    \n",
    "    def __init__(self, pos='positive_senti.csv', neg='negative_senti.csv'):\n",
    "        self.pos = pos\n",
    "        self.neg = neg\n",
    "        self.path  = ''\n",
    "        noise = stopwords.words('russian') + list(punctuation)\n",
    "        upnoise = [letter.upper() for letter in noise]\n",
    "        self.sum_noise = noise+upnoise+['.','»','«', 'Коллега', \"коллега\", \"это\",'спасибо', \n",
    "                           'такой',\"уважаемый\", \"квартира\", \"который\", \"свой\", \"пожалуйста\"]\n",
    "        \n",
    "    def token_text(self, text):\n",
    "        return [word for word in word_tokenize(text.lower()) if word not in self.sum_noise]\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        text = text.lower().replace(\"ё\", \"е\")\n",
    "        text = text.lower().replace(\"USER\", \"\")\n",
    "        text = text.lower().replace(\"rt\", \"\")\n",
    "        text = text.lower().replace(\"URL\", \"\")\n",
    "        text = text.lower().replace(\"\", \"\")\n",
    "        text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', text)\n",
    "        text = re.sub('@[^\\s]+', 'USER', text)\n",
    "        text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)\n",
    "        text = re.sub(' +', ' ', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def create_data(self):\n",
    "        n = ['id', 'date', 'name', 'text', 'typr', 'rep', 'rtw', 'faw', 'stcount', 'foll', 'frien', 'listcount']\n",
    "        data_positive = pd.read_csv(self.pos, sep=';', error_bad_lines=False, names=n, usecols=['text'])\n",
    "        data_negative = pd.read_csv(self.neg, sep=';', error_bad_lines=False, names=n, usecols=['text'])\n",
    "\n",
    "        # Формируем сбалансированный датасет\n",
    "        sample_size = min(data_positive.shape[0], data_negative.shape[0])\n",
    "        raw_data = np.concatenate((data_positive['text'].values[:sample_size],\n",
    "                                   data_negative['text'].values[:sample_size]), axis=0)\n",
    "        self.labels = [1] * sample_size + [0] * sample_size\n",
    "        \n",
    "        \n",
    "        self.data = [self.preprocess_text(t) for t in raw_data]\n",
    "        self.data_tok = pd.Series(self.data).apply(self.token_text)\n",
    "\n",
    "        self.WORDS = set()\n",
    "        for sent in self.data_tok:\n",
    "            for w in sent:\n",
    "                self.WORDS.add(w)\n",
    "                \n",
    "                \n",
    "        df_train=pd.DataFrame(columns=['Text', 'Label'])\n",
    "        df_test=pd.DataFrame(columns=['Text', 'Label'])\n",
    "\n",
    "        df_train['Text'], df_test['Text'], df_train['Label'], df_test['Label'] = train_test_split(self.data,\n",
    "                                                                                                  self.labels,\n",
    "                                                                                                  test_size=0.2,\n",
    "                                                                                                  random_state=1)\n",
    "        \n",
    "        df_val=pd.DataFrame(columns=['Text', 'Label'])\n",
    "        self.df_train, self.df_val = train_test_split(df_train, test_size=0.2, random_state=1)\n",
    "        \n",
    "        print('data created')\n",
    "        \n",
    "    def fit(self):\n",
    "        \n",
    "        self.create_data()\n",
    "        \n",
    "        \n",
    "        \n",
    "        tokenizer=Tokenizer(lang='xx')\n",
    "        data_lm = fastai.text.data.TextLMDataBunch.from_df(self.path, tokenizer=tokenizer,\n",
    "                                                   bs=16, train_df=self.df_train, valid_df=self.df_val,\n",
    "                                                           text_cols=0)\n",
    "        print('batches formed')\n",
    "        \n",
    "        data_test_clas = fastai.text.data.TextClasDataBunch.from_df(self.path, vocab=data_lm.train_ds.vocab,\n",
    "                                                            bs=32, train_df=self.df_train, valid_df=self.df_val,\n",
    "                                                            text_cols=0, label_cols=1, tokenizer=tokenizer)\n",
    "        \n",
    "        \n",
    "        config = fastai.text.models.awd_lstm_clas_config.copy()\n",
    "        config['n_hid'] = 1150\n",
    "        self.learn_test =text_classifier_learner(data_test_clas, AWD_LSTM, config=config, drop_mult=0.5)\n",
    "        \n",
    "        self.learn_test.load_encoder('/home/victor/fb/ft_enc')\n",
    "        self.learn_test.load('/home/victor/fb/tw_lstm')\n",
    "        \n",
    "        print('model learned')\n",
    "        \n",
    "    def predict(self, obj:'str'):\n",
    "        \n",
    "        return self.learn_test.predict(obj)\n",
    "'''\n",
    "Это анализатор текстов Вити Виталича\n",
    "первое и наверно самое главное - формат данных\n",
    "это должен быть датафрейм в пандасе\n",
    "там должны обязательно быть 3 колонки, с которыми происходит вся работы \n",
    "и называться они должны только так и никак иначе \"post\", \"comment\" и \"date\"\n",
    "соответственно это посты комменты и даты\n",
    "важно в каком формате все должно быть\n",
    "колонка post должна содержать str строку \n",
    "колонка comment это жуткий костыль:\n",
    "пустые комменты: вот так '[]', то есть это строка с двумя скобками как будто это лист\n",
    "непустые комменты вот так \"['2 млн лучше брать в кредит', 'У меня была аналогичная ситуация']\"\n",
    "то есть это строка, внутри которой как будто бы лист из строк через запятую)\n",
    "date - это дата и она должна быть в формате таймдейт\n",
    "\n",
    "Ну поехали\n",
    "вызывать это вот так\n",
    "test = TextAnalyzer(df), где df - это данные, в формате как я указал выше\n",
    "Если пользоваться моим парсером, то они уже такие \n",
    "\n",
    "Функции:\n",
    "\n",
    "1.frequency(numb, text_type, start_date = None, end_date = None, ngram = 1, draw=True)\n",
    "это частота слов, то есть сколько таких слов было за период\n",
    "1. в нее нужно передать обязательно число топ-слов которые ты хочешь вывести\n",
    "2. обязательно тип данных - коммент или пост\n",
    "Необязательные параметры:\n",
    "1. дата начала в формате листа [день,месяц,год]\n",
    "2. дата конца в таком же формате\n",
    "если их не передавать, то берется статистика по всем имеющимся датам\n",
    "3. словосочетания. По умолчанию смотрит одно слово, можно указать искать встречаемость словосочетаний \n",
    "из 2,3 и тд\n",
    "4. отрисовка, если мешают графики можно вырубить\n",
    "\n",
    "Пример:\n",
    "test.frequency(20, 'comment', start_date=[28,9,2020], ngram=2,draw=False)\n",
    "он выведет 20 самых частых словосочетаний из 2 слов из комментов начиная с 28 сентября 20 года без графика\n",
    "\n",
    "2.date_top(text_type, nmb, start_date = None, end_date = None, draw = False)\n",
    "это топ слово рядом с датой\n",
    "обязательные параметры:\n",
    "1. тип данных. см пред ф-ию\n",
    "2. количество топ слов по этой дате \n",
    "необязательные\n",
    "дата начала и  конца, см пред. ф-ию\n",
    "Пример:\n",
    "test.date_top('comment', 1) - 1 самое популярное слово в комментах на каждый день\n",
    "\n",
    "3. word_in_time(word)\n",
    "интерактивный график использования слова во времени\n",
    "Параметр 1 - слово которое хочешь смотретт\n",
    "Пример:\n",
    "test.word_in_time('право') - увидишь график слова \"право\" во времени\n",
    "\n",
    "4.date_unique(nmb, start_date = None, end_date = None)\n",
    "Это отдельная функция, которая рассчитана на то, что ты указываешь даты\n",
    "то есть ее бесполезно применять на всем промежутке\n",
    "смысл ее в том, что она выделяет слова, которые значимы именно для этих дат,\n",
    "относительно всего корпуса текстов\n",
    "\n",
    "обязательные параметры:\n",
    "1. количество таких топ-слов\n",
    "Необязательные\n",
    "1.дата начала\n",
    "2.дата конца\n",
    "\n",
    "Пример:\n",
    "test.date_unique(15, start_date=[3,10,2020]) - 15 наиболее важных слов для периода 3.10.20-нынешняя дата\n",
    "\n",
    "5.context(word, start_date = None, end_date = None, k = 15, ngram = 1, draw=True)\n",
    "Эта штука смотрит на контекст слова, но смотрит довольно втупую, то есть какое слово часто идет рядом с\n",
    "тем, которое ты указала\n",
    "Обязательные параметры:\n",
    "1. Слово)\n",
    "Необязательные\n",
    "1.дата начала\n",
    "2. дата конца\n",
    "3. количество слов контекста, по умолчанию 15. А также этот параметр регулирует окно, то есть сколько слов\n",
    "смотри влево и вправо от твоего указанного в каждом предложении\n",
    "4. словосочетания. По умолчанию смотрит одно слово, можно указать искать встречаемость словосочетаний \n",
    "из 2,3 и тд\n",
    "5. отрисовка - если не хочешь график выключи\n",
    "\n",
    "Пример:\n",
    "test.context('право', k=25) - 25 самых частых слов контекста у слова \"право\"\n",
    "\n",
    "6.visualise_context(keys, n, start_date=None, end_date=None, min_cnt = 5, wind = 20, \n",
    "                         draw=True, title='Context Visualisation', a=0.7)\n",
    "Эта штука находит контексты умнее и рисует их на графике. \n",
    "чем ближе слова на графике, тем ближе они по контексту\n",
    "обязательные параметры:\n",
    "1. слова для которых ищется контекст, они должны быть в формате листа из строк ['слово', \"второе слово\"]\n",
    "2. количество слов контекста требуемых\n",
    "необязательные параметры:\n",
    "1. дата начала и дата конца\n",
    "2.рисовать или нет\n",
    "3. название графика title\n",
    "4. остальные тяжело обьяснить, просто не трогай их\n",
    "Пример:\n",
    "test.visualise_context(['залог'], 15) - нарисован график на котором слово \"залог\" и 15 контекстных слов\n",
    "'''  \n",
    "class TextAnalyzer:\n",
    "    def __init__(self, df):\n",
    "        noise = stopwords.words('russian') + list(punctuation)\n",
    "        upnoise = [letter.upper() for letter in noise]\n",
    "        self.sum_noise = noise+upnoise+['.','»','«', 'Коллега', \"коллега\", \"это\",'спасибо', \n",
    "                           'такой',\"уважаемый\", \"квартира\", \"который\", \"свой\", \"пожалуйста\"]\n",
    "        ##########TIME#######################\n",
    "        for i in range(len(df)):\n",
    "            try:\n",
    "                df['time'][i] = datetime.strptime(df[\"time\"][i], '%Y-%m-%d')\n",
    "            except TypeError:\n",
    "                continue\n",
    "            \n",
    "        self.df = deepcopy(self.tokeniz(df))\n",
    "        self.df2 = deepcopy(df)\n",
    "        \n",
    "    @protected\n",
    "    def tokeniz(self, df):\n",
    "\n",
    "        #########COMMENTS#####################\n",
    "        for i in range(len(df)):\n",
    "            df[\"comment\"][i] = list(df[\"comment\"][i][2:-2].replace(\"'\", '').split(','))\n",
    "        tw = TweetTokenizer()\n",
    "        det = TreebankWordDetokenizer()\n",
    "        for i in (range(len(df))):\n",
    "            for j in range(len(df[\"comment\"][i])):\n",
    "                tokenized_example = (tw.tokenize(df[\"comment\"][i][j]))\n",
    "                filtered_example = [word for word in tokenized_example if not word in self.sum_noise]\n",
    "                df[\"comment\"][i][j] = det.detokenize(filtered_example)\n",
    "        mystem_analyzer = Mystem(entire_input=False)\n",
    "        for i in (range(len(df))):\n",
    "            df[\"comment\"][i] = [mystem_analyzer.lemmatize(w) for w in df[\"comment\"][i]]\n",
    "            df[\"comment\"][i] = list(filter(None, df[\"comment\"][i]))\n",
    "        for i in range(len(df)):\n",
    "            for j in range(len(df['comment'][i])):\n",
    "                df['comment'][i][j] = [word for word in df['comment'][i][j] if not word in self.sum_noise]\n",
    "\n",
    "\n",
    "        ##########POSTS##############\n",
    "        for i in (range(len(df))):\n",
    "                tokenized_example = (tw.tokenize(df[\"post\"][i]))\n",
    "                filtered_example = [word for word in tokenized_example if not word in self.sum_noise]\n",
    "                df[\"post\"][i] = det.detokenize(filtered_example)\n",
    "        for i in (range(len(df))):\n",
    "            a = []\n",
    "            a.append(df['post'][i])\n",
    "            df[\"post\"][i] = a\n",
    "        for i in (range(len(df))):\n",
    "            df[\"post\"][i] = [mystem_analyzer.lemmatize(w) for w in df[\"post\"][i]][0]\n",
    "        for i in range(len(df)):\n",
    "            df['post'][i] = [word for word in df['post'][i] if not word in self.sum_noise]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @protected\n",
    "    def date_slice(self, start_date = None, end_date = None):\n",
    "        if start_date == None and end_date == None:\n",
    "                self.df1 = deepcopy(self.df)\n",
    "        \n",
    "        if (type(start_date) == list or type(end_date) == list):\n",
    "            if (not start_date == None ) and (not end_date==None):\n",
    "                start_date = datetime(start_date[2], start_date[1], start_date[0], 0, 0)\n",
    "                end_date = datetime(end_date[2], end_date[1], end_date[0], 0, 0)\n",
    "                self.df1 = self.df[self.df['time']<end_date]\n",
    "                self.df1 = self.df1[self.df1['time']>start_date]\n",
    "            elif (not start_date == None ) and (end_date==None):\n",
    "                start_date = datetime(start_date[2], start_date[1], start_date[0], 0, 0)\n",
    "                self.df1 = self.df[self.df['time']>start_date]\n",
    "            elif (start_date == None ) and (not end_date==None):\n",
    "                end_date = datetime(end_date[2], end_date[1], end_date[0], 0, 0)\n",
    "                self.df1 = self.df[self.df['time']<end_date]\n",
    "        elif (type(start_date) == datetime  and type(end_date) == datetime):\n",
    "            self.df1 = self.df[self.df['time']<end_date]\n",
    "            self.df1 = self.df1[self.df1['time']>=start_date]\n",
    "            \n",
    "    @protected\n",
    "    def date_slice1(self, df, start_date = None, end_date = None):\n",
    "        if start_date == None and end_date == None:\n",
    "                self.df1 = deepcopy(df)\n",
    "        \n",
    "        if (type(start_date) == list or type(end_date) == list):\n",
    "            if (not start_date == None ) and (not end_date==None):\n",
    "                start_date = datetime(start_date[2], start_date[1], start_date[0], 0, 0)\n",
    "                end_date = datetime(end_date[2], end_date[1], end_date[0], 0, 0)\n",
    "                self.df1 = df[df['time']<end_date]\n",
    "                self.df1 = self.df1[self.df1['time']>start_date]\n",
    "            elif (not start_date == None ) and (end_date==None):\n",
    "                start_date = datetime(start_date[2], start_date[1], start_date[0], 0, 0)\n",
    "                self.df1 = df[df['time']>start_date]\n",
    "            elif (start_date == None ) and (not end_date==None):\n",
    "                end_date = datetime(end_date[2], end_date[1], end_date[0], 0, 0)\n",
    "                self.df1 = df[df['time']<end_date]\n",
    "        elif (type(start_date) == datetime  and type(end_date) == datetime):\n",
    "            self.df1 = df[df['time']<end_date]\n",
    "            self.df1 = self.df1[self.df1['time']>=start_date]\n",
    "            \n",
    "    def frequency(self,  numb, text_type, start_date = None, end_date = None, ngram = 1, draw=True, output=True):\n",
    "        #start&end date = [d, m ,y]\n",
    "        if ngram>1:\n",
    "            draw = False\n",
    "        self.numb = numb\n",
    "        self.wordcount = {}\n",
    "        self.text_type = text_type\n",
    "        self.draw = draw\n",
    "        #self.start_date = start_date\n",
    "        #self.end_date = end_date\n",
    "       \n",
    "        self.date_slice1(self.df, start_date, end_date)\n",
    "        \n",
    "        if self.text_type == 'comment':\n",
    "            \n",
    "            self.df1.index = np.arange(len(self.df1))\n",
    "            if ngram>1:\n",
    "                for i in range(len(self.df1)):\n",
    "                    for j in range(len(self.df1['comment'][i])):\n",
    "                        self.df1['comment'][i][j] = list(ngrams(self.df1['comment'][i][j],ngram))\n",
    "            for i in range(len(self.df1)):\n",
    "                for j in range(len(self.df1['comment'][i])):\n",
    "                    for word in (self.df1['comment'][i][j]):\n",
    "                       # if ngram==1:\n",
    "                        #    word = word.replace(\".\",\"\")\n",
    "                         #   word = word.replace(\",\",\"\")\n",
    "                          #  word = word.replace(\":\",\"\")\n",
    "                           # word = word.replace(\"\\\"\",\"\")\n",
    "                           # word = word.replace(\"!\",\"\")\n",
    "                           # word = word.replace(\"â€œ\",\"\")\n",
    "                           # word = word.replace(\"â€˜\",\"\")\n",
    "                           # word = word.replace(\"*\",\"\")\n",
    "                           # word = word.replace(\" \",\"\")\n",
    "                        if word not in self.sum_noise:\n",
    "                            if word not in self.wordcount.keys():\n",
    "                                self.wordcount[word] = 1\n",
    "                            else:\n",
    "                                self.wordcount[word] += 1\n",
    "        if self.text_type == 'post':\n",
    "            self.df1.index = np.arange(len(self.df1))\n",
    "            if ngram>1:\n",
    "                for i in range(len(self.df1)):\n",
    "                    self.df1['post'][i] = list(ngrams(self.df1['post'][i],ngram))\n",
    "            for i in range(len(self.df1)):   \n",
    "                for word in self.df1['post'][i]:\n",
    "                  #  if ngram==1:\n",
    "                   #     word = word.replace(\".\",\"\")\n",
    "                    ##    word = word.replace(\",\",\"\")\n",
    "                     #   word = word.replace(\":\",\"\")\n",
    "                     #   word = word.replace(\"\\\"\",\"\")\n",
    "                     #   word = word.replace(\"!\",\"\")\n",
    "                     #   word = word.replace(\"â€œ\",\"\")\n",
    "                     #   word = word.replace(\"â€˜\",\"\")\n",
    "                     #   word = word.replace(\"*\",\"\")\n",
    "                     #   word = word.replace(\" \",\"\")\n",
    "                    if word not in self.sum_noise:\n",
    "                        if word not in self.wordcount.keys():\n",
    "                            self.wordcount[word] = 1\n",
    "                        else:\n",
    "                            self.wordcount[word] += 1\n",
    "        self.word_counter = collections.Counter(self.wordcount)\n",
    "        if output:\n",
    "            print(\"{} наиболее часто встречающихся слов\\n\".format(self.numb))\n",
    "            for word, count in self.word_counter.most_common(self.numb):\n",
    "                print(word, \": \", count)\n",
    "        if self.draw:\n",
    "            lst = self.word_counter.most_common(self.numb)\n",
    "            df_cnt = pd.DataFrame(lst, columns = ['Word', 'Count'])\n",
    "            plt.figure(figsize=(20, 12))\n",
    "            plt.bar(df_cnt['Word'], df_cnt['Count'])\n",
    "        if ngram>1:\n",
    "            global df \n",
    "            df = pd.read_csv(\"fb.csv\", index_col=0)\n",
    "            tokeniz(df)\n",
    "            self.df = df\n",
    "    def date_top(self, text_type, nmb, start_date = None, end_date = None, draw = False):\n",
    "        self.date_slice1(self.df, start_date, end_date)\n",
    "            \n",
    "        self.dates = np.sort(self.df1['time'].unique())\n",
    "        self.top_words = []\n",
    "        print(\"Самое частое слово за день\\n\")\n",
    "        for i in range(len(self.dates)):\n",
    "            self.frequency(nmb, text_type, start_date = self.dates[i], end_date = self.dates[i]+timedelta(days=1), draw = draw, output=False)\n",
    "            #self.top_words.append(self.word_counter.most_common(1)[0][0])\n",
    "            if not len(self.word_counter.most_common(nmb)) == 0:\n",
    "                print(self.word_counter.most_common(nmb), self.dates[i].date(), '\\n')\n",
    "            else:\n",
    "                print('no comments', self.dates[i].date(), '\\n')\n",
    "    def word_in_time(self, word):\n",
    "        self.word = word\n",
    "        self.dates = np.sort(self.df['time'].unique())\n",
    "        self.wrd_cnt = []\n",
    "        for i in range(len(self.dates)):\n",
    "            self.count = 0\n",
    "            self.dfwrd = self.df[self.df['time'] == self.dates[i]]\n",
    "            self.dfwrd.index = np.arange(len(self.dfwrd))\n",
    "            for j in range(len(self.dfwrd)):\n",
    "                for word1 in self.dfwrd['post'][j]:\n",
    "                    if word1 == self.word:\n",
    "                        self.count +=1\n",
    "                for comm in self.dfwrd['comment'][j]:\n",
    "                    for word2 in comm:\n",
    "                        if word2 == self.word:\n",
    "                            self.count +=1\n",
    "            self.wrd_cnt.append(self.count)\n",
    "        self.plots = pd.DataFrame(self.wrd_cnt, columns=['Количество упоминаний'])\n",
    "        self.plots['Дата'] = self.dates\n",
    "        f = plt.figure(figsize=(19, 15))\n",
    "        fig = px.line(self.plots, x='Дата', y = 'Количество упоминаний', title = 'Встречаемость слова \"{}\" во времени'.format(word))\n",
    "        fig.update_xaxes(\n",
    "            rangeslider_visible=True,\n",
    "            rangeselector=dict(\n",
    "                buttons=list([\n",
    "                    dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n",
    "                    dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n",
    "                    dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n",
    "                    dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n",
    "                    dict(step=\"all\")\n",
    "                ])\n",
    "            )\n",
    "        )\n",
    "        fig.show()\n",
    "        \n",
    "    def date_unique(self, nmb, start_date = None, end_date = None):\n",
    "        self.date_slice1(self.df, start_date, end_date)\n",
    "        self.df1_nd = deepcopy(self.df.iloc[np.delete(np.arange(len(self.df)), self.df1.index),:])\n",
    "        self.df1.index = np.arange(len(self.df1))\n",
    "        self.nmb = nmb\n",
    "        self.d = ''\n",
    "        det = TreebankWordDetokenizer()\n",
    "        for i in range(len(self.df1)):\n",
    "            self.d += ' ' + det.detokenize(self.df1['post'][i])\n",
    "        for i in range(len(self.df1)):\n",
    "            for j in range(len(self.df1['comment'][i])):\n",
    "                self.d += ' ' + det.detokenize(self.df1['comment'][i][j])\n",
    "        self.df1_nd.index = np.arange(len(self.df1_nd))\n",
    "        self.nd = ''\n",
    "        for i in range(len(self.df1_nd)):\n",
    "            self.nd += ' ' + det.detokenize(self.df1_nd['post'][i])\n",
    "        for i in range(len(self.df1_nd)):\n",
    "            for j in range(len(self.df1_nd['comment'][i])):\n",
    "                self.nd += ' ' + det.detokenize(self.df1_nd['comment'][i][j])\n",
    "        self.ls_dt = [self.d, self.nd]\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        self.X = vectorizer.fit_transform(self.ls_dt)\n",
    "        self.tt = pd.DataFrame(self.X.toarray(), columns=vectorizer.get_feature_names())\n",
    "        self.dick = dict(zip(self.tt.columns, self.tt.loc[0]))\n",
    "        self.utp = sorted(self.dick, key=self.dick.get, reverse=True)\n",
    "        print ('{} самых значимых слов для данного промежутка\\n'.format(self.nmb))\n",
    "        for i in range(nmb):\n",
    "            print('{}:'.format(i+1)+' '+self.utp[i])\n",
    "    def context(self, word, start_date = None, end_date = None, k = 15, ngram = 1, draw=True):\n",
    "        self.date_slice1(self.df, start_date, end_date)\n",
    "        self.closest = []\n",
    "        if ngram ==1:     \n",
    "            for i in range(len(self.df1)):\n",
    "                j = 0\n",
    "                try:\n",
    "                    index = self.df1['post'][i][j:].index(word)\n",
    "                    if index>k:\n",
    "                        if len(self.df1['post'][i][j:][index:])>k:\n",
    "                            context_ls = self.df1['post'][i][j:][index-k:index+k]\n",
    "                        elif len(self.df1['post'][i][j:][index:])<=k:\n",
    "                            context_ls = self.df1['post'][i][j:][index-k:]\n",
    "                    elif index<=k:\n",
    "                        if len(self.df1['post'][i][j:][index:])>k:\n",
    "                            context_ls = self.df1['post'][i][j:][:index+k]\n",
    "                        elif len(self.df1['post'][i][j:][index:])<=k:\n",
    "                            context_ls = self.df1['post'][i][j:][:]\n",
    "                    self.closest.append(context_ls)\n",
    "                    j = index\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            for i in range(len(self.df1)):\n",
    "                for f in range(len(self.df1['comment'][i])):\n",
    "                    j = 0\n",
    "                    try:\n",
    "                        index = self.df1['comment'][i][f][j:].index(word)\n",
    "                        if index>k:\n",
    "                            if len(self.df1['comment'][i][f][j:][index:])>k:\n",
    "                                context_ls = self.df1['comment'][i][f][j:][index-k:index+k]\n",
    "                            elif len(self.df1['comment'][i][f][j:][index:])<=k:\n",
    "                                context_ls = self.df1['comment'][i][f][j:][index-k:]\n",
    "                        elif index<=k:\n",
    "                            if len(self.df1['comment'][i][f][j:][index:])>k:\n",
    "                                context_ls = self.df1['comment'][i][f][j:][:index+k]\n",
    "                            elif len(self.df1['comment'][i][f][j:][index:])<=k:\n",
    "                                context_ls = self.df1['comment'][i][f][j:][:]\n",
    "                        self.closest.append(context_ls)\n",
    "                        j = index\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "            self.wordcount_contxt = {}\n",
    "            for i in range(len(self.closest)):   \n",
    "                        for word in self.closest[i]:\n",
    "                            word = word.replace(\".\",\"\")\n",
    "                            word = word.replace(\",\",\"\")\n",
    "                            word = word.replace(\":\",\"\")\n",
    "                            word = word.replace(\"\\\"\",\"\")\n",
    "                            word = word.replace(\"!\",\"\")\n",
    "                            word = word.replace(\"â€œ\",\"\")\n",
    "                            word = word.replace(\"â€˜\",\"\")\n",
    "                            word = word.replace(\"*\",\"\")\n",
    "                            if word not in self.sum_noise:\n",
    "                                if word not in self.wordcount_contxt:\n",
    "                                    self.wordcount_contxt[word] = 1\n",
    "                                else:\n",
    "                                    self.wordcount_contxt[word] += 1\n",
    "            print(\"{} наиболее часто встречающихся слов контекста\\n\".format(k))\n",
    "            self.wordcount_contxt.pop(word)\n",
    "            word_counter = collections.Counter(self.wordcount_contxt)\n",
    "            for word, count in word_counter.most_common(k):\n",
    "                   print(word, \": \", count)\n",
    "                \n",
    "        if ngram>1:\n",
    "            print('не работает, извините(')\n",
    "            draw=False\n",
    "            '''\n",
    "            \n",
    "            for i in range(len(self.df1)):\n",
    "                    for j in range(len(self.df1['comment'][i])):\n",
    "                        self.df1['comment'][i][j] = list(ngrams(self.df1['comment'][i][j],ngram))\n",
    "            for i in range(len(self.df1)):\n",
    "                    self.df1['post'][i] = list(ngrams(self.df1['post'][i],ngram))\n",
    "                    \n",
    "            self.contxt_ls = []\n",
    "            for i in range(len(self.df1)):\n",
    "                for j in range(len(self.df1['post'][i])):\n",
    "                    if word in self.df1['post'][i][j]:\n",
    "                        self.contxt_ls.append(self.df1['post'][i][j])\n",
    "            for i in range(len(self.df1)):\n",
    "                for j in range(len(self.df1['comment'][i])):\n",
    "                    for f in range(len(self.df1['comment'][i][j])):\n",
    "                        if word in self.df1['comment'][i][j][f]:\n",
    "                            self.contxt_ls.append(self.df1['comment'][i][j][f])\n",
    "            self.wordcount_contxt = {}\n",
    "            for word in (self.contxt_ls):   \n",
    "                if word not in self.sum_noise:\n",
    "                    if word not in self.wordcount_contxt:\n",
    "                        self.wordcount_contxt[word] = 1\n",
    "                    else:\n",
    "                        self.wordcount_contxt[word] += 1\n",
    "            self.wordcount_contxt.pop(word)\n",
    "            print(\"{} наиболее часто встречающихся слов контекста\\n\".format(k))\n",
    "            word_counter = collections.Counter(self.wordcount_contxt)\n",
    "            for word, count in word_counter.most_common(k):\n",
    "                   print(word, \": \", count) \n",
    "            \n",
    "            global df\n",
    "            df = pd.read_csv(\"fb.csv\", index_col=0)\n",
    "            tokeniz(df)\n",
    "            self.df = deepcopy(df)\n",
    "            '''\n",
    "        if draw:\n",
    "            lst = word_counter.most_common(k)\n",
    "            df_cnt = pd.DataFrame(lst, columns = ['Word', 'Count'])\n",
    "            plt.figure(figsize=(20, 12))\n",
    "            plt.bar(df_cnt['Word'], df_cnt['Count'])\n",
    "\n",
    "    def visualise_context(self, keys:\"list\", n, start_date=None, end_date=None, min_cnt = 5, wind = 20, \n",
    "                         draw=True, title='Context Visualisation', a=0.7):\n",
    "        \n",
    "        self.date_slice1(self.df2, start_date, end_date)\n",
    "        \n",
    "        corpus = []\n",
    "        for ls in self.df1['post']:\n",
    "            corpus.append(ls)\n",
    "        for i in range(len(self.df1)):\n",
    "            for j in range(len(self.df1['comment'][i])):\n",
    "                corpus.append(self.df1['comment'][i][j])\n",
    "                \n",
    "        \n",
    "        self.keys = keys\n",
    "        model = gensim.models.word2vec.Word2Vec(sentences = corpus, min_count = min_cnt, window=wind,\n",
    "                                                workers=4)\n",
    "\n",
    "        embedding_clusters = []\n",
    "        word_clusters = []\n",
    "        for word in keys:\n",
    "            embeddings = []\n",
    "            words = []\n",
    "            for similar_word, _ in model.most_similar(word, topn=n):\n",
    "                words.append(similar_word)\n",
    "                embeddings.append(model[similar_word])\n",
    "            words.append(word)\n",
    "            embeddings.append(model[word])\n",
    "            embedding_clusters.append(embeddings)\n",
    "            word_clusters.append(words)\n",
    "        embedding_clusters = np.array(embedding_clusters)\n",
    "        n, m, k = embedding_clusters.shape\n",
    "        try:\n",
    "            tsne_model_en_2d = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=32)\n",
    "            embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n",
    "        except:\n",
    "            tsne_model_en_2d = TSNE(perplexity=15, n_components=2, n_iter=3500, random_state=32)\n",
    "            embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n",
    "        \n",
    "        if draw:\n",
    "            \n",
    "            plt.figure(figsize=(20, 12))\n",
    "            colors = cm.rainbow(np.linspace(0, 1, len(self.keys)))\n",
    "            for label, embeddings, words, color in zip(self.keys, embeddings_en_2d, word_clusters, colors):\n",
    "                x = embeddings[:, 0]\n",
    "                y = embeddings[:, 1]\n",
    "                plt.scatter(x, y, c=color.reshape(1,-1), alpha=a, label=label)\n",
    "                for i, word in enumerate(words):\n",
    "                    plt.annotate(word, alpha=3, xy=(x[i], y[i]), xytext=(5, 2),\n",
    "                                 textcoords='offset points', ha='right', va='bottom', size=15)\n",
    "            plt.legend(loc=4)\n",
    "            plt.title(title)\n",
    "            plt.grid(True)\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
